# Vision-Based Pursuit-Evasion Control: PID vs Deep RL

A comprehensive research project comparing classical control (PID, Kalman Filter + PID) with modern Deep Reinforcement Learning (SAC) for vision-based pursuit-evasion tasks.

## ğŸ“‹ Project Overview

This project implements a 2D pursuit-evasion simulation to benchmark different control strategies. The challenge: an agent must learn to track a target moving with Brownian motion using only egocentric, vision-based observations (64Ã—64 stacked frames).

### Key Features

- **Custom Gymnasium Environment**: Dynamic physics with acceleration-based control
- **Three Control Approaches**:
  1. **PID Controller**: Classical control with OpenCV-based visual detection
  2. **Kalman Filter + PID**: State estimation for robust tracking
  3. **SAC Deep RL**: End-to-end learning from pixels
- **Comprehensive Evaluation**: Metrics, visualizations, and statistical comparison
- **Publication-Ready**: Automated figure generation and LaTeX table export

## ğŸ—ï¸ Project Structure

```
PIDRL/
â”œâ”€â”€ environments/           # Custom Gymnasium environment
â”‚   â”œâ”€â”€ pursuit_evasion_env.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ controllers/            # Classical controllers
â”‚   â”œâ”€â”€ pid_controller.py
â”‚   â”œâ”€â”€ kalman_filter.py
â”‚   â”œâ”€â”€ kalman_pid_controller.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ agents/                 # Deep RL agents
â”‚   â”œâ”€â”€ networks.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                  # Utilities
â”‚   â”œâ”€â”€ visual_detection.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ experiments/            # Training and evaluation scripts
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ compare_methods.py
â”œâ”€â”€ configs/                # Configuration files
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ demo.py                 # Interactive demo
â”œâ”€â”€ test_environment.py     # Quick test script
â””â”€â”€ requirements.txt        # Dependencies
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- CUDA (optional, for GPU acceleration)

### Install Dependencies

```bash
# Clone the repository
cd PIDRL

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Verify Installation

```bash
python test_environment.py
```

## ğŸ® Quick Start

### 1. Demo Visualization

Test each controller interactively:

```bash
# PID Controller
python demo.py pid --n-episodes 3

# Kalman Filter + PID
python demo.py kalman-pid --n-episodes 3

# SAC Agent (requires trained model)
python demo.py sac --sac-model models/sac/final_model --n-episodes 3
```

### 2. Train SAC Agent

```bash
python experiments/train_sac.py \
    --config configs/config.yaml \
    --save-dir models/sac \
    --tensorboard-log logs/sac
```

Monitor training with TensorBoard:

```bash
tensorboard --logdir logs/sac
```

### 3. Evaluate All Methods

```bash
python experiments/evaluate.py \
    --config configs/config.yaml \
    --sac-model models/sac/final_model \
    --n-episodes 100 \
    --output-dir results
```

### 4. Generate Comparison Plots

```bash
python experiments/compare_methods.py \
    --config configs/config.yaml \
    --sac-model models/sac/final_model \
    --n-episodes 100 \
    --output-dir results
```

This will generate:
- Performance comparison plots
- Statistical analysis
- LaTeX tables for the paper
- CSV files with detailed results

## ğŸ“Š Evaluation Metrics

The framework computes the following metrics:

1. **Mean Episode Reward**: Cumulative reward per episode
2. **Tracking Error**: Mean squared distance to target
3. **Success Rate**: Percentage of time target is within threshold
4. **Episode Length**: Steps per episode
5. **Detection Rate**: Vision system reliability (PID/Kalman-PID only)

## ğŸ”§ Configuration

Edit `configs/config.yaml` to customize:

- Environment parameters (physics, observation space)
- PID gains (Kp, Ki, Kd)
- Kalman Filter noise parameters
- SAC hyperparameters (learning rate, network architecture)
- Evaluation settings

## ğŸ“ˆ Experimental Workflow

### Full Research Pipeline

```bash
# 1. Train SAC agent
python experiments/train_sac.py --config configs/config.yaml

# 2. Evaluate all methods and generate plots
python experiments/compare_methods.py \
    --config configs/config.yaml \
    --sac-model models/sac/final_model \
    --n-episodes 100

# 3. Results are saved to results/
#    - results/evaluation_summary.csv
#    - results/figures/*.png
#    - results/performance_table.tex
```

## ğŸ§ª Testing

Run quick tests to verify components:

```bash
# Test environment
python test_environment.py

# Test PID controller
python -c "from controllers import PIDAgent; print('PID OK')"

# Test Kalman Filter
python -c "from controllers import KalmanFilter; print('Kalman OK')"

# Test SAC networks
python -c "from agents.networks import CustomCNN; print('Networks OK')"
```

## ğŸ“ Research Paper Integration

### Generated Assets

After running experiments, you'll have:

1. **Figures** (`results/figures/`):
   - `reward_comparison.png`
   - `tracking_error_comparison.png`
   - `success_rate_comparison.png`
   - `distance_over_time_ep*.png`

2. **LaTeX Table** (`results/performance_table.tex`):
   - Ready-to-include performance comparison table

3. **Data** (`results/*/`):
   - NumPy arrays for custom analysis
   - CSV files for spreadsheet analysis

### Citing

If you use this code in your research, please cite:

```bibtex
@article{yourname2024pidrl,
  title={Vision-Based Pursuit-Evasion Control: Comparing Classical and Deep Reinforcement Learning Approaches},
  author={Your Name},
  journal={Your Journal/Conference},
  year={2024}
}
```

## ğŸ› ï¸ Customization

### Adding New Controllers

1. Implement controller in `controllers/`
2. Add agent wrapper with `predict()` method
3. Update `experiments/evaluate.py` to include new method

### Modifying Environment

Edit `environments/pursuit_evasion_env.py`:
- Change observation space
- Adjust physics parameters
- Modify reward function
- Add new features

### Tuning Hyperparameters

Use `configs/config.yaml`:
```yaml
pid:
  kp: 0.5    # Proportional gain
  ki: 0.01   # Integral gain
  kd: 0.2    # Derivative gain

sac:
  learning_rate: 3.0e-4
  buffer_size: 100000
  batch_size: 256
```

## ğŸ› Troubleshooting

### Common Issues

**Issue**: `pygame.error: No available video device`
- **Solution**: Set `render_mode=None` or use virtual display

**Issue**: CUDA out of memory
- **Solution**: Reduce `batch_size` in SAC config

**Issue**: Slow training
- **Solution**: Reduce `frame_size` or `buffer_size`

## ğŸ“š Dependencies

Key libraries:
- **Gymnasium**: Environment API
- **Stable-Baselines3**: SAC implementation
- **PyTorch**: Deep learning
- **OpenCV**: Computer vision
- **Pygame**: Rendering
- **Matplotlib/Seaborn**: Visualization

See `requirements.txt` for complete list.

## ğŸ¯ Research Questions

This project helps answer:

1. How does end-to-end RL compare to classical control?
2. Does Kalman filtering improve PID performance?
3. What is the sample efficiency trade-off?
4. How do methods generalize to different motion patterns?

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Additional controllers (MPC, LQR)
- More RL algorithms (PPO, TD3, DQN)
- 3D environments
- Multi-agent scenarios
- Real-world deployment

## ğŸ“„ License

This project is licensed under the MIT License. See `LICENSE` for details.

## ğŸ™ Acknowledgments

- OpenAI Gym/Gymnasium for the RL framework
- Stable-Baselines3 for SAC implementation
- OpenCV community for computer vision tools

## ğŸ“§ Contact

For questions or collaboration:
- Open an issue on GitHub
- Email: [your-email@example.com]

---

**Happy Researching! ğŸš€**
