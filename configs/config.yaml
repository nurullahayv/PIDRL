# Environment Configuration
environment:
  # Observation settings
  frame_size: 64  # Width and height of each frame
  frame_stack: 4  # Number of frames to stack

  # Physics settings
  dt: 0.1  # Time step in seconds
  max_velocity: 25.0  # Maximum velocity magnitude for agent (INCREASED for faster action)
  max_acceleration: 3.0  # Maximum acceleration magnitude (INCREASED for responsive control)
  max_angular_velocity: 3.5  # Maximum turning rate (rad/s) - increased agility
  friction: 0.92  # Velocity damping factor (lower = faster deceleration)

  # World bounds
  world_size: 100.0  # Size of the world (symmetric around origin)
  view_size: 30.0  # Square field of view size (half-width, replaces view_radius)

  # 2.5D / 3D Settings (for depth perception)
  depth_range: [10.0, 50.0]  # [min_depth, max_depth] in world units
  num_targets: 1  # Number of targets to track (multi-target capability)

  # Target settings
  target_brownian_std: 4.0  # Standard deviation of Brownian motion (INCREASED for more erratic movement)
  target_evasion_strength: 2.0  # How strongly targets evade from agent (INCREASED for aggressive evasion)
  target_max_speed_ratio: 1.0  # Target max speed as ratio of agent max speed (1.0 = equal speed!)
  target_size: 7.0  # Base radius of target in world units (LARGER for easier lock-on)

  # Agent settings
  agent_size: 1.5  # Radius of agent in world units

  # Episode settings
  max_steps: 500
  success_threshold: 9.0  # 30% of view_size (30.0 * 0.3 = 9.0) - inner square focus area

  # Reward settings
  reward_scale: 0.01  # Scale factor for distance-based reward

# Training Scenarios
scenarios:
  scenario_1:
    name: "RL Agent vs Random Target"
    agent_type: "rl"  # DQN, PPO, SAC, or TD3
    target_type: "random"  # Random movement

  scenario_2:
    name: "Competitive MARL: RL Pursuer vs RL Evader"
    agent_type: "rl"  # Pursuer (DQN, PPO, SAC, or TD3)
    target_type: "rl"  # Evader (DQN, PPO, SAC, or TD3)

# DQN Configuration (Discrete actions)
dqn:
  # Network architecture
  cnn_features: [32, 64, 64]  # Conv layer channels
  cnn_kernels: [8, 4, 3]  # Kernel sizes
  cnn_strides: [4, 2, 1]  # Strides
  mlp_units: [256, 256]  # MLP hidden units

  # Training hyperparameters
  learning_rate: 1.0e-4
  buffer_size: 100000
  batch_size: 128
  gamma: 0.99  # Discount factor
  target_update_interval: 10000  # Update target network every N steps
  exploration_fraction: 0.3  # Fraction of training for epsilon decay
  exploration_initial_eps: 1.0  # Initial epsilon
  exploration_final_eps: 0.05  # Final epsilon

  # Training settings
  total_timesteps: 500000
  learning_starts: 10000
  train_freq: 4
  gradient_steps: 1

  # Evaluation
  eval_freq: 10000
  n_eval_episodes: 10

# PPO Configuration (Policy Gradient)
ppo:
  # Network architecture
  cnn_features: [32, 64, 64]  # Conv layer channels
  cnn_kernels: [8, 4, 3]  # Kernel sizes
  cnn_strides: [4, 2, 1]  # Strides
  mlp_units: [256, 256]  # MLP hidden units

  # Training hyperparameters
  learning_rate: 3.0e-4
  n_steps: 2048  # Steps per update
  batch_size: 128
  n_epochs: 10  # Optimization epochs per update
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda
  clip_range: 0.2  # PPO clip range
  ent_coef: 0.01  # Entropy coefficient

  # Training settings
  total_timesteps: 1000000  # PPO needs more steps

  # Evaluation
  eval_freq: 20000
  n_eval_episodes: 10

# SAC Configuration (Actor-Critic, Off-Policy)
sac:
  # Network architecture
  cnn_features: [32, 64, 64]  # Conv layer channels
  cnn_kernels: [8, 4, 3]  # Kernel sizes
  cnn_strides: [4, 2, 1]  # Strides
  mlp_units: [256, 256]  # MLP hidden units

  # Training hyperparameters
  learning_rate: 3.0e-4
  buffer_size: 200000  # Larger buffer for better sample diversity
  batch_size: 256
  gamma: 0.99  # Discount factor
  tau: 0.005  # Target network update rate
  ent_coef: "auto"  # Auto-tune entropy coefficient

  # Training settings
  total_timesteps: 500000
  learning_starts: 10000
  train_freq: 1
  gradient_steps: 1

  # Evaluation
  eval_freq: 10000
  n_eval_episodes: 10

# TD3 Configuration (Twin Delayed DDPG, Actor-Critic, Off-Policy)
td3:
  # Network architecture
  cnn_features: [32, 64, 64]  # Conv layer channels
  cnn_kernels: [8, 4, 3]  # Kernel sizes
  cnn_strides: [4, 2, 1]  # Strides
  mlp_units: [256, 256]  # MLP hidden units

  # Training hyperparameters
  learning_rate: 3.0e-4
  buffer_size: 200000
  batch_size: 256
  gamma: 0.99  # Discount factor
  tau: 0.005  # Target network update rate
  policy_delay: 2  # Delay policy update
  target_policy_noise: 0.2  # Noise added to target policy
  target_noise_clip: 0.5  # Clip target policy noise

  # Training settings
  total_timesteps: 500000
  learning_starts: 10000
  train_freq: 1
  gradient_steps: 1

  # Evaluation
  eval_freq: 10000
  n_eval_episodes: 10

# Experiment Configuration
experiment:
  seed: 42
  n_runs: 5  # Number of runs for statistical significance
  save_dir: "results"
  render_eval: false
  record_video: true
  log_interval: 1000
