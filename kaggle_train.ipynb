{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ PIDRL: Competitive Pursuit-Evasion with Deep RL\n",
    "\n",
    "Train a Deep RL agent to track agile targets in 3D egocentric environment.\n",
    "\n",
    "**Features:**\n",
    "- 3D pursuit-evasion with depth perception\n",
    "- Focus-based reward system\n",
    "- Competitive MARL ready (agent vs target)\n",
    "- GPU accelerated training\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** Enable GPU in Kaggle:\n",
    "- Settings ‚Üí Accelerator ‚Üí GPU T4 x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/nurullahayv/PIDRL.git\n",
    "%cd PIDRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run setup script (installs dependencies and checks GPU)\n",
    "!python setup_kaggle.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test run (~1 minute)\n",
    "!python quick_train.py --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training (500k steps, ~2 hours with GPU)\n",
    "!python quick_train.py --full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor Training (Run in Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/sac_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best model (no rendering in Kaggle)\n",
    "!python test_trained_model.py --model models/sac_full/best_model/best_model.zip --episodes 10 --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip model for download\n",
    "!zip -r trained_model.zip models/sac_full/best_model\n",
    "\n",
    "# You can download trained_model.zip from the Output section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import SAC\n",
    "import yaml\n",
    "from environments import make_env\n",
    "import numpy as np\n",
    "\n",
    "# Load config\n",
    "with open('configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load model\n",
    "model = SAC.load(\"models/sac_full/best_model/best_model.zip\")\n",
    "\n",
    "# Test episodes\n",
    "env = make_env(config, use_3d=True)\n",
    "\n",
    "episode_rewards = []\n",
    "focus_times = []\n",
    "\n",
    "for _ in range(50):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    total_focus_time = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        if info.get('in_focus', False):\n",
    "            total_focus_time += 1\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    focus_times.append(total_focus_time / steps * 100)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.hist(episode_rewards, bins=20, edgecolor='black')\n",
    "ax1.set_title('Episode Rewards Distribution')\n",
    "ax1.set_xlabel('Total Reward')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(np.mean(episode_rewards), color='red', linestyle='--', label=f'Mean: {np.mean(episode_rewards):.2f}')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(focus_times, bins=20, edgecolor='black')\n",
    "ax2.set_title('Time in Focus Distribution')\n",
    "ax2.set_xlabel('Focus Time (%)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(np.mean(focus_times), color='red', linestyle='--', label=f'Mean: {np.mean(focus_times):.1f}%')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResults (50 episodes):\")\n",
    "print(f\"  Average Reward: {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"  Average Focus Time: {np.mean(focus_times):.1f}% ¬± {np.std(focus_times):.1f}%\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "Your trained model is saved in `models/sac_full/best_model/`\n",
    "\n",
    "Download `trained_model.zip` from the Output section to use locally."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
