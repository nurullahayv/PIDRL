# PIDRL Project Roadmap - 5 AÅŸamalÄ± GeliÅŸtirme PlanÄ±

## ğŸ¯ Proje Vizyonu

Hierarchical Multi-Agent Reinforcement Learning sistemi ile dogfight simÃ¼lasyonu.

---

## ğŸ“‹ AÅAMA 1: Temel Competitive 3D Pursuit-Evasion

### Hedef
- Sadece 3D environment (2D sistemler kaldÄ±rÄ±lacak)
- Agent: RL ile takip (pursuer)
- Target: PID ile kaÃ§Ä±ÅŸ (evader - karenin dÄ±ÅŸÄ±na Ã§Ä±kmaya Ã§alÄ±ÅŸÄ±r)
- Her ikisi de acceleration vectÃ¶rÃ¼ Ã¼retir
- Relative motion: target_acc - agent_acc

### YapÄ±lacaklar
- [ ] 2D environment ve ilgili dosyalarÄ± kaldÄ±r
  - `environments/pursuit_evasion_env.py` â†’ SÄ°L
  - `demo.py` â†’ SÄ°L (zaten silindi)
  - 2D PID controllers â†’ SÄ°L
- [ ] 3D environment'Ä± gÃ¼ncelle
  - Agent ve target iÃ§in acceleration vektÃ¶rleri
  - Relative motion hesaplama
- [ ] Target iÃ§in PID evader controller
  - AmaÃ§: Karenin kenarÄ±na git ve dÄ±ÅŸarÄ± Ã§Ä±k
  - Input: Kendi pozisyonu
  - Output: Acceleration vectÃ¶r (ax, ay, az)
- [ ] Agent iÃ§in RL training
  - SAC algoritmasÄ±
  - Reward: Target'Ä± FOV iÃ§inde tut
- [ ] Test ve visualizasyon

### Dosya YapÄ±sÄ±
```
environments/
  â””â”€â”€ pursuit_evasion_env_3d.py  (ana environment)
controllers/
  â”œâ”€â”€ pid_controller_3d.py        (agent iÃ§in - opsiyonel)
  â”œâ”€â”€ kalman_pid_controller_3d.py (agent iÃ§in - opsiyonel)
  â””â”€â”€ target_evader_pid.py        (NEW - target iÃ§in)
agents/
  â””â”€â”€ sac_agent.py                (agent iÃ§in RL)
```

### BaÅŸarÄ± Kriteri
- âœ… Agent RL ile target'Ä± takip edebiliyor
- âœ… Target PID ile kaÃ§abiliyor (dÄ±ÅŸarÄ± Ã§Ä±kmaya Ã§alÄ±ÅŸÄ±yor)
- âœ… Competitive reward sistemi Ã§alÄ±ÅŸÄ±yor
- âœ… Training stabil ve etkili

**Tahmini SÃ¼re**: 1-2 gÃ¼n

---

## ğŸ“‹ AÅAMA 2: Target iÃ§in RL Agent (Competitive MARL)

### Hedef
- Target da RL ile Ã¶ÄŸrenir (SAC)
- Ä°ki RL agent birbirine karÅŸÄ± (adversarial training)
- Self-play veya population-based training

### YapÄ±lacaklar
- [ ] Target iÃ§in RL agent
  - SAC agent (agent ile aynÄ±)
  - Reward: Agent'tan kaÃ§, FOV dÄ±ÅŸÄ±nda kal
- [ ] Training pipeline
  - Self-play: Ä°ki agent birlikte eÄŸitiliyor
  - Curriculum learning: Kolay â†’ zor
- [ ] Multi-agent training
  - Parallel environments
  - Experience sharing (opsiyonel)
- [ ] Evaluation
  - Agent vs Agent
  - Performance metrics (escape rate, capture rate)

### Dosya YapÄ±sÄ±
```
agents/
  â”œâ”€â”€ sac_agent.py
  â””â”€â”€ multi_agent_trainer.py  (NEW - competitive training)
experiments/
  â””â”€â”€ train_competitive.py     (NEW - self-play training)
```

### BaÅŸarÄ± Kriteri
- âœ… Her iki agent de RL ile Ã¶ÄŸreniyor
- âœ… Adversarial training stabil
- âœ… Agent'lar gittikÃ§e geliÅŸiyor (arms race)
- âœ… Win rate ~50% civarÄ±nda dengelenmiÅŸ

**Tahmini SÃ¼re**: 2-3 gÃ¼n

---

## ğŸ“‹ AÅAMA 3: 3D Arena + Search & Pursuit Modes

### Hedef
- GeniÅŸ 3D arena (Ã¶rn: 1000x1000x1000 birim)
- UÃ§ak modelleri veya kÃ¼reler
- Ä°ki mod:
  1. **Search mode**: Birbirini gÃ¶rmÃ¼yorlar, arama yapÄ±yorlar
  2. **Pursuit mode**: FOV'a girince takip baÅŸlÄ±yor
- Realistic FOV (cone-based, limited range)

### YapÄ±lacaklar
- [ ] 3D Arena environment
  - BÃ¼yÃ¼k hareket alanÄ±
  - 3D pozisyon ve yÃ¶nelim (position + orientation)
  - UÃ§ak fizik modeli (yaw, pitch, roll)
- [ ] FOV sistemi
  - Cone-based gÃ¶rÃ¼ÅŸ alanÄ± (azimuth, elevation)
  - Range limitation
  - Visibility check
- [ ] Search behavior
  - Random search pattern
  - Intelligent search (RL-based veya rule-based)
  - Sensor modeling
- [ ] Mode switching
  - Search â†’ Pursuit (target detected)
  - Pursuit â†’ Search (target lost)
- [ ] Visualization
  - 3D rendering (pygame 3D veya OpenGL)
  - UÃ§ak modelleri
  - FOV cone gÃ¶sterimi

### Dosya YapÄ±sÄ±
```
environments/
  â”œâ”€â”€ arena_3d.py           (NEW - bÃ¼yÃ¼k 3D arena)
  â”œâ”€â”€ pursuit_mode.py       (mevcut pursuit-evasion refactor)
  â””â”€â”€ search_mode.py        (NEW - search behavior)
utils/
  â”œâ”€â”€ fov_cone.py           (NEW - cone-based FOV)
  â”œâ”€â”€ aircraft_model.py     (NEW - uÃ§ak fizik)
  â””â”€â”€ visibility.py         (NEW - visibility check)
rendering/
  â”œâ”€â”€ renderer_3d.py        (NEW - 3D visualization)
  â””â”€â”€ assets/               (NEW - uÃ§ak modelleri)
```

### BaÅŸarÄ± Kriteri
- âœ… Agent'lar bÃ¼yÃ¼k arenada hareket edebiliyor
- âœ… Search mode'da birbirini bulabiliyor
- âœ… Pursuit mode'a geÃ§iÅŸ smooth
- âœ… 3D gÃ¶rselleÅŸtirme Ã§alÄ±ÅŸÄ±yor

**Tahmini SÃ¼re**: 3-4 gÃ¼n

---

## ğŸ“‹ AÅAMA 4: Multi-Agent Dogfight (N vs N)

### Hedef
- Ã‡ok sayÄ±da uÃ§ak (Ã¶rn: 4 vs 4 veya free-for-all)
- Her agent diÄŸerlerini kitlemeye Ã§alÄ±ÅŸÄ±r
- Target selection (hangi dÃ¼ÅŸmanÄ± takip edeceÄŸine karar ver)
- Hierarchical decision making
- Her agent iÃ§in ayrÄ± takip ekranÄ±

### YapÄ±lacaklar
- [ ] Multi-agent environment
  - N agent desteÄŸi
  - Global state + local observations
  - Collision detection
- [ ] Target selection
  - High-level policy: Hangi hedefi seÃ§?
  - Factors: Mesafe, gÃ¶rÃ¼ÅŸ aÃ§Ä±sÄ±, tehdit seviyesi
- [ ] Team coordination (opsiyonel)
  - Communication
  - Formation flying
- [ ] Hierarchical structure
  - High-level: Taktik karar (hangi hedef?)
  - Low-level: Takip kontrolÃ¼ (pursuit-evasion)
- [ ] Multi-screen visualization
  - Her agent iÃ§in split-screen
  - 4-6 agent iÃ§in grid layout
  - Real-time switch between views

### Dosya YapÄ±sÄ±
```
environments/
  â””â”€â”€ multi_agent_dogfight.py  (NEW - N vs N)
agents/
  â”œâ”€â”€ hierarchical_agent.py    (NEW - high + low level)
  â””â”€â”€ target_selector.py       (NEW - target selection)
rendering/
  â””â”€â”€ multi_view_renderer.py   (NEW - split screen)
experiments/
  â””â”€â”€ train_multi_agent.py     (NEW - multi-agent training)
```

### BaÅŸarÄ± Kriteri
- âœ… N agent aynÄ± anda Ã§alÄ±ÅŸÄ±yor
- âœ… Target selection akÄ±llÄ±ca yapÄ±lÄ±yor
- âœ… Agent'lar engage/disengage kararÄ± verebiliyor
- âœ… Multi-screen gÃ¶rselleÅŸtirme Ã§alÄ±ÅŸÄ±yor

**Tahmini SÃ¼re**: 4-5 gÃ¼n

---

## ğŸ“‹ AÅAMA 5: Hierarchical RL + No-Fly Zones

### Hedef
- HRL sistemi:
  - High-level: Strateji (attack, evade, patrol, reposition)
  - Mid-level: Taktik (target selection, maneuver type)
  - Low-level: Motor control (pursuit-evasion skills)
- No-fly zones: Hava savunma sistemleri (SAM sites)
- BÃ¼yÃ¼k harita

### YapÄ±lacaklar
- [ ] HRL architecture
  - Options framework veya Feudal RL
  - High-level policy (abstract actions)
  - Low-level policies (primitive skills)
- [ ] Strategic behaviors
  - **Attack**: DÃ¼ÅŸmana yaklaÅŸ ve engage et
  - **Evade**: Tehlikeden kaÃ§
  - **Patrol**: AlanÄ± koru
  - **Reposition**: AvantajlÄ± pozisyon al
- [ ] No-fly zones
  - YarÄ±m kÃ¼re ÅŸeklinde yasak bÃ¶lgeler
  - SAM sistemi modeling (detection range, firing)
  - Penalty for entering
- [ ] Map design
  - Stratejik noktalar (waypoints)
  - Terrain (opsiyonel - daÄŸlar, vadiler)
  - Multiple no-fly zones
- [ ] Training
  - Curriculum learning (basit â†’ kompleks)
  - Multi-task learning
  - Transfer learning (low-level skills reuse)

### Dosya YapÄ±sÄ±
```
agents/
  â”œâ”€â”€ hrl_agent.py              (NEW - hierarchical agent)
  â”œâ”€â”€ high_level_policy.py      (NEW - strategy)
  â”œâ”€â”€ mid_level_policy.py       (NEW - tactics)
  â””â”€â”€ low_level_policies/       (pursuit, evade, etc.)
environments/
  â”œâ”€â”€ strategic_dogfight.py     (NEW - full system)
  â””â”€â”€ no_fly_zones.py           (NEW - SAM systems)
utils/
  â””â”€â”€ map_generator.py          (NEW - map creation)
experiments/
  â””â”€â”€ train_hrl.py              (NEW - HRL training)
```

### BaÅŸarÄ± Kriteri
- âœ… Agent'lar stratejik kararlar verebiliyor
- âœ… No-fly zone'lardan kaÃ§Ä±nÄ±yor
- âœ… Hierarchical policies etkili Ã§alÄ±ÅŸÄ±yor
- âœ… Complex scenarios'larda baÅŸarÄ±lÄ±

**Tahmini SÃ¼re**: 5-7 gÃ¼n

---

## ğŸ“Š Genel Zaman Ã‡izelgesi

| AÅŸama | AÃ§Ä±klama | Tahmini SÃ¼re | KÃ¼mÃ¼latif |
|-------|----------|--------------|-----------|
| **AÅŸama 1** | Competitive 3D (RL vs PID) | 1-2 gÃ¼n | 2 gÃ¼n |
| **AÅŸama 2** | Target RL (RL vs RL) | 2-3 gÃ¼n | 5 gÃ¼n |
| **AÅŸama 3** | 3D Arena + Search/Pursuit | 3-4 gÃ¼n | 9 gÃ¼n |
| **AÅŸama 4** | Multi-Agent Dogfight | 4-5 gÃ¼n | 14 gÃ¼n |
| **AÅŸama 5** | HRL + No-Fly Zones | 5-7 gÃ¼n | 21 gÃ¼n |

**Toplam**: ~3 hafta (yoÄŸun Ã§alÄ±ÅŸma ile)

---

## ğŸ”§ Teknik Detaylar

### Ortak BileÅŸenler
- **RL Algorithm**: SAC (Soft Actor-Critic)
  - Continuous action space
  - Off-policy learning
  - Sample efficient
- **Framework**: Stable-Baselines3
- **Rendering**: Pygame (2D HUD) + OpenGL/PyVista (3D arena)
- **Physics**: Custom (simplified aircraft dynamics)

### Her AÅŸama Ä°Ã§in
- Training scripts
- Evaluation scripts
- Visualization tools
- Unit tests
- Documentation

---

## ğŸ“ Ã–ÄŸrenme DeÄŸeri

Bu proje ÅŸunlarÄ± Ã¶ÄŸretir:
1. **AÅŸama 1-2**: Competitive MARL, self-play
2. **AÅŸama 3**: State machines, mode switching
3. **AÅŸama 4**: Multi-agent coordination, target selection
4. **AÅŸama 5**: Hierarchical RL, strategic decision making

---

## ğŸš€ YaklaÅŸÄ±m

### GeliÅŸtirme SÄ±rasÄ±
1. âœ… Her aÅŸamayÄ± sÄ±rayla tamamla
2. âœ… Her aÅŸama sonunda test ve validation
3. âœ… Bir sonraki aÅŸamaya geÃ§meden Ã¶nce stabil hale getir
4. âœ… Git branch'leri kullan (phase-1, phase-2, etc.)

### Branch Stratejisi
```
main
â”œâ”€â”€ phase-1-competitive-3d
â”œâ”€â”€ phase-2-competitive-marl
â”œâ”€â”€ phase-3-arena-search
â”œâ”€â”€ phase-4-multi-agent
â””â”€â”€ phase-5-hrl
```

### Iterative Development
- Her aÅŸamada MVP (Minimum Viable Product) yaklaÅŸÄ±mÄ±
- Ã–nce Ã§alÄ±ÅŸÄ±r hale getir, sonra optimize et
- Continuous testing

---

## ğŸ“ Sonraki AdÄ±m

**ÅÄ°MDÄ°**: AÅŸama 1'i baÅŸlatalÄ±m!

OnayÄ±nÄ±zÄ± bekliyorum. AÅŸama 1'e baÅŸlayalÄ±m mÄ±?

### AÅŸama 1 Checklist:
- [ ] 2D dosyalarÄ± temizle
- [ ] Target evader PID controller yaz
- [ ] 3D environment'Ä± gÃ¼ncelle (acceleration vectÃ¶rleri)
- [ ] Training pipeline kur
- [ ] Test ve demo

**Tahmini SÃ¼re**: 1-2 gÃ¼n
**BaÅŸlamak iÃ§in onay bekliyor**: âœ‹
